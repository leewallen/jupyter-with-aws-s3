{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Data From S3\n",
    "\n",
    "This example shows how to configure a JupyterLab docker image to access data from AWS S3.\n",
    "\n",
    "## Build a Docker Image with AWS Related JARs\n",
    "\n",
    "First, we need to build a docker image that includes the missing jars files needed for accessing S3. You can also add the jars using a volume mount, and then include code in your notebook to update the `PYSPARK_SUBMIT_ARGS` to include the jars from their location within the docker image. I felt like baking the jars into the docker image was a little easier that having to run a code cell to update the `PYSPARK_SUBMIT_ARGS`.\n",
    "\n",
    "This example is using Spark 3.0.1 with Hadoop 3.2, and the files that we're adding are:\n",
    "\n",
    "* aws-java-sdk-bundle-1.11.950.jar\n",
    "* hadoop-aws-3.2.0.jar\n",
    "* jets3t-0.9.4.jar\n",
    "\n",
    "Here is an example Dockerfile to use:\n",
    "\n",
    "```\n",
    "FROM jupyter/pyspark-notebook:8ea7abc5b7bc\n",
    "\n",
    "USER root\n",
    "\n",
    "ENV PYSPARK_SUBMIT_ARGS '--packages com.amazonaws:aws-java-sdk:1.11.950,org.apache.hadoop:hadoop-aws:3.2.0,net.java.dev.jets3t:jets3t:0.9.4 pyspark-shell'\n",
    "\n",
    "\n",
    "# Download missing jars\n",
    "\n",
    "# Get AWS SDK JAR\n",
    "RUN (cd /usr/local/spark/jars && curl -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.950/aws-java-sdk-bundle-1.11.950.jar)\n",
    "\n",
    "# Get Hadoop-AWS Jar\n",
    "RUN (cd /usr/local/spark/jars && curl -O https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar)\n",
    "\n",
    "# Get jets3t JAR\n",
    "RUN (cd /usr/local/spark/jars && curl -O https://repo1.maven.org/maven2/net/java/dev/jets3t/jets3t/0.9.4/jets3t-0.9.4.jar)\n",
    "\n",
    "USER $NB_UID\n",
    "```\n",
    "\n",
    "\n",
    "## Run the Docker Container and Pass in AWS Credentials\n",
    "\n",
    "This example is assuming that you have appropriate credentials saved in $HOME/.aws/credentials, and have jq installed.\n",
    "\n",
    "Fetch temporary credentials from AWS and run the docker container with the credentials and session token passed in as environment variables:\n",
    "\n",
    "```bash\n",
    "creds_json=$(aws --profile default --region us-west-2 sts get-session-token)\n",
    "\n",
    "docker run -d --name jupyter --rm -p 8888:8888 \\\n",
    "  -e AWS_ACCESS_KEY_ID=$(echo \"$creds_json\" | jq -r .Credentials.AccessKeyId) \\\n",
    "  -e AWS_SECRET_ACCESS_KEY=$(echo \"$creds_json\" | jq -r .Credentials.SecretAccessKey) \\\n",
    "  -e AWS_SESSION_TOKEN=$(echo \"$creds_json\" | jq -r .Credentials.SessionToken) \\\n",
    "  jupyter-docker:yourtag jupyter lab --LabApp.token ''\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the SparkSession Thread Count and Memory\n",
    "\n",
    "If you have JupyterLab running in the cloud, and you can afford to run enough instances where you're not overly concerned with cost, then don't worry about this section. If you are running JupyterLab on a single machine (for example, a laptop with limited resources), and the amount of data you want to process is more than you have available on the machine, then you might want to be thoughtful about how you initialize the SparkSession. If the single machine (perhaps your home laptop) use case sounds like you, then this is what I considered when configuring the SparkSession.\n",
    "\n",
    "I have 8 cores and 16GB of memory available on my laptop, and I configured Docker to use 4 cores and up to 3GB of memory. \n",
    "\n",
    "\n",
    "Things to consider if the Spark cluster is on a constrained system:\n",
    "\n",
    "* How much memory do you have available for your Spark job?\n",
    "    > If you don't have much memory available, then consider reading the [Spark Memory Tuning Guide](https://spark.apache.org/docs/latest/tuning.html#memory-tuning). There are great suggestions for everything from changing the default serializer to being aware of the impacts of using broadcast variables.\n",
    "* How much data do you plan to process? \n",
    "    > You also might want to be aware of the format that your source data is in. [Here is a nice article comparing CSV, JSON, and Parquet](https://www.linkedin.com/pulse/spark-file-format-showdown-csv-vs-json-parquet-garren-staubli/). If your data is in JSON, but you want to process the data as Parquet, then consider creating a job to convert the data to Parquet before using the data in your processing jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"2g\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"Covid19TimeSeries\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config(\"fs.s3a.path.style.access\", True) \\\n",
    "    .config(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\") \\\n",
    "    .config(\"fs.s3a.endpoint\", \"s3.us-west-2.amazonaws.com\") \\\n",
    "    .config(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"com.amazonaws.services.s3.enableV4\", True) \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data From S3\n",
    "\n",
    "At this point you should be able to read data in from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = \"s3a://dev-leewallen-spark/covid-19-time-series/parquet/covid-19.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetDF = spark.read.parquet(s3path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------+------+--------------+---------+\n",
      "|Confirmed|Country/Region|      Date|Deaths|Province/State|Recovered|\n",
      "+---------+--------------+----------+------+--------------+---------+\n",
      "|        0|   Afghanistan|2020-01-22|     0|          null|        0|\n",
      "|        0|   Afghanistan|2020-01-23|     0|          null|        0|\n",
      "|        0|   Afghanistan|2020-01-24|     0|          null|        0|\n",
      "|        0|   Afghanistan|2020-01-25|     0|          null|        0|\n",
      "|        0|   Afghanistan|2020-01-26|     0|          null|        0|\n",
      "+---------+--------------+----------+------+--------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "usConfirmed = parquetDF.filter((col('`Country/Region`') == \"US\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chart_studio.plotly as py\n",
    "# import plotly.graph_objects as go\n",
    "# from  plotly.offline import plot\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------+------+--------------+---------+----------+\n",
      "|Confirmed|Country/Region|      Date|Deaths|Province/State|Recovered|    DateTS|\n",
      "+---------+--------------+----------+------+--------------+---------+----------+\n",
      "|        1|            US|2020-01-22|     0|          null|        0|2020-01-22|\n",
      "|        1|            US|2020-01-23|     0|          null|        0|2020-01-23|\n",
      "|        2|            US|2020-01-24|     0|          null|        0|2020-01-24|\n",
      "|        2|            US|2020-01-25|     0|          null|        0|2020-01-25|\n",
      "|        5|            US|2020-01-26|     0|          null|        0|2020-01-26|\n",
      "+---------+--------------+----------+------+--------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usConfirmed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Confirmed: long (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Deaths: long (nullable = true)\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Recovered: long (nullable = true)\n",
      " |-- DateTS: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usConfirmed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "usConfirmed = usConfirmed.withColumn(\"DateTS\",usConfirmed[\"Date\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------+------+--------------+---------+----------+\n",
      "|Confirmed|Country/Region|      Date|Deaths|Province/State|Recovered|    DateTS|\n",
      "+---------+--------------+----------+------+--------------+---------+----------+\n",
      "|        1|            US|2020-01-22|     0|          null|        0|2020-01-22|\n",
      "|        1|            US|2020-01-23|     0|          null|        0|2020-01-23|\n",
      "|        2|            US|2020-01-24|     0|          null|        0|2020-01-24|\n",
      "|        2|            US|2020-01-25|     0|          null|        0|2020-01-25|\n",
      "|        5|            US|2020-01-26|     0|          null|        0|2020-01-26|\n",
      "|        5|            US|2020-01-27|     0|          null|        0|2020-01-27|\n",
      "|        5|            US|2020-01-28|     0|          null|        0|2020-01-28|\n",
      "|        6|            US|2020-01-29|     0|          null|        0|2020-01-29|\n",
      "|        6|            US|2020-01-30|     0|          null|        0|2020-01-30|\n",
      "|        8|            US|2020-01-31|     0|          null|        0|2020-01-31|\n",
      "|        8|            US|2020-02-01|     0|          null|        0|2020-02-01|\n",
      "|        8|            US|2020-02-02|     0|          null|        0|2020-02-02|\n",
      "|       11|            US|2020-02-03|     0|          null|        0|2020-02-03|\n",
      "|       11|            US|2020-02-04|     0|          null|        0|2020-02-04|\n",
      "|       11|            US|2020-02-05|     0|          null|        0|2020-02-05|\n",
      "|       12|            US|2020-02-06|     0|          null|        0|2020-02-06|\n",
      "|       12|            US|2020-02-07|     0|          null|        0|2020-02-07|\n",
      "|       12|            US|2020-02-08|     0|          null|        0|2020-02-08|\n",
      "|       12|            US|2020-02-09|     0|          null|        3|2020-02-09|\n",
      "|       12|            US|2020-02-10|     0|          null|        3|2020-02-10|\n",
      "+---------+--------------+----------+------+--------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Confirmed: long (nullable = true)\n",
      " |-- Country/Region: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Deaths: long (nullable = true)\n",
      " |-- Province/State: string (nullable = true)\n",
      " |-- Recovered: long (nullable = true)\n",
      " |-- DateTS: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usConfirmed.show()\n",
    "usConfirmed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "usPandas = usConfirmed.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Date</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Recovered</th>\n",
       "      <th>DateTS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-25</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US</td>\n",
       "      <td>2020-01-26</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>25147891</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-24</td>\n",
       "      <td>419251</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>25298986</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-25</td>\n",
       "      <td>421168</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>25445583</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-26</td>\n",
       "      <td>425252</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>25598061</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-27</td>\n",
       "      <td>429195</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>25762726</td>\n",
       "      <td>US</td>\n",
       "      <td>2021-01-28</td>\n",
       "      <td>433067</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>373 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Confirmed Country/Region        Date  Deaths Province/State  Recovered  \\\n",
       "0            1             US  2020-01-22       0           None          0   \n",
       "1            1             US  2020-01-23       0           None          0   \n",
       "2            2             US  2020-01-24       0           None          0   \n",
       "3            2             US  2020-01-25       0           None          0   \n",
       "4            5             US  2020-01-26       0           None          0   \n",
       "..         ...            ...         ...     ...            ...        ...   \n",
       "368   25147891             US  2021-01-24  419251           None          0   \n",
       "369   25298986             US  2021-01-25  421168           None          0   \n",
       "370   25445583             US  2021-01-26  425252           None          0   \n",
       "371   25598061             US  2021-01-27  429195           None          0   \n",
       "372   25762726             US  2021-01-28  433067           None          0   \n",
       "\n",
       "         DateTS  \n",
       "0    2020-01-22  \n",
       "1    2020-01-23  \n",
       "2    2020-01-24  \n",
       "3    2020-01-25  \n",
       "4    2020-01-26  \n",
       "..          ...  \n",
       "368  2021-01-24  \n",
       "369  2021-01-25  \n",
       "370  2021-01-26  \n",
       "371  2021-01-27  \n",
       "372  2021-01-28  \n",
       "\n",
       "[373 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usPandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Plot Related Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.plotting.matplotlib.register_converters = True\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Plot Interactive\n",
    "\n",
    "Make the plot resizeable, and provide an interface so you can save your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11dd89fff574e2db3b672ec774d8c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usPandas.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648837246b954244845293a387096e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usPandas.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
